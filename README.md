# Author Detection 

# Idea
Throughout history, literature has played a vast and important role in nearly every aspect of human life. Books have inspired entertainment in the form of music, films, artwork -- just to name a few -- as well as laying out grand ideas for the research and development of new systems and inventions. However, in this day and age, it seems that there is a trend where less people are reading written literature [4], and after a period of being forced to read in their primary education, they might cease reading altogether later on.
We can potentially acknowledge that this trend may in part be caused by the advent of algorithmically designed extremely captivating social media, as well as other forms of entertainment (visual media for example). However, if we can recommend literature for people that they can truly enjoy, maybe we can alleviate this problem. Ths in itself is not necessarily a novel idea: literature is often recommended based on genres, historical time periods, works by the same author, etc. However, there have been few tools in the past to analyze author styles. Thus, we introduce the idea of a model to detect similarities in author styles.

While the original idea of author similarity detection was developed independently, seeing some of the work that the Stony Book team is doing has inspired some of the methods that I would like to try [3]. I would also like to thank Naoya Inoue for providing some information regarding the potential dataset, as well as showing me his paper about character representations in novels, which gave me a better sense of how to approach the evaluation of my proposed project. 

# Dataset
Project Gutenberg books and all associated metadata, specifically fragments of different books, by different as well as the same authors. [1, 2]

# Implementation
Our basic idea is to use a pre-trained model such as BERT, and train it on book fragments from each author. In our model, the input data would be the various book fragments, from which we’d take grammatical embeddings (just one idea) and the model would calculate a score for the author based on these embeddings thus utilizing the grammar analysis capabilities of BERT [6]. We can then use these scores to determine the similarities between the aforementioned authors.

# Evaluation
We’ve already discussed the idea of comparing the scores generated by our model trained on the different authors’ book fragments. However, we could describe additional tasks to test for the accuracy of our learned models such as starting with some baseline detection of genre for one.

# Challenges
With respect to the actual idea, it is definitely a bit abstract, and would require real world experiments to determine the efficacy of this idea. While it is a nice thought that people would enjoy literature given that they have different authors with similar styles, we don’t have a concrete ground truth.
With respect to the implementation, as with essentially all tasks involving transformer-based models, it is perhaps computationally expensive to be able to analyze more and more data (in the form of multiple fragments from books in this project). Furthermore, I personally don’t have a lot of experience with all the different types of strategies that could be employed in this project. While I currently have a method of scoring based on grammatical embeddings, there could well be a better or easier method to accomplish what I’m trying to accomplish. 

# Citations:
1. https://gutenberg.org/
2. https://arxiv.org/abs/1812.08092
3. https://stonybook.org/
4. https://www.emerald.com/insight/content/doi/10.1108/00220410510632040/full/pdf?title=reading-behavior-in-the-digital-environment-changes-in-reading-behavior-over-the-past-ten-years
5. https://arxiv.org/abs/1909.02597
7. https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2760185.pdf
8. https://github.com/arthtalati/Deep-Learning-based-Authorship-Identification